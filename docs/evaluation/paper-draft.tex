\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}

\newcommand{\TODO}[1]{\textcolor{red}{\textbf{[TODO: #1]}}}
\newcommand{\PLACEHOLDER}[1]{\textcolor{orange}{\textbf{[#1]}}}

\begin{document}

\title{Automatic Domain Randomization for Robust Sim-to-Real Transfer in Locomotion Tasks}

\author{\IEEEauthorblockN{Marc'Antonio Lopez}
\IEEEauthorblockA{\textit{Department of Control and Computer Engineering} \\
\textit{Polytechnic University of Turin}\\
Turin, Italy \\
s336362@studenti.polito.it}
\and
\IEEEauthorblockN{Luigi Marguglio}
\IEEEauthorblockA{\textit{Department of Control and Computer Engineering} \\
\textit{Polytechnic University of Turin}\\
Turin, Italy \\
\PLACEHOLDER{email}}
}

\maketitle

\begin{abstract}
Transferring reinforcement learning policies from simulation to the real world remains a fundamental challenge due to the mismatch between simulated and physical dynamics, commonly known as the reality gap. In this work, we implement and evaluate Automatic Domain Randomization for the MuJoCo Hopper locomotion task. Unlike traditional approaches that use fixed randomization ranges, our method adaptively expands environmental difficulty based on agent performance, creating a natural curriculum that prevents both underfitting and learned helplessness. Our experiments demonstrate that this approach achieves robustness to variations of up to 70\% in physical parameters including mass, damping, and friction. Most notably, the trained policy exhibits an inverse transfer gap, where performance on the target domain actually exceeds that on the source domain by 65\%, indicating exceptional generalization capabilities.
\end{abstract}

\begin{IEEEkeywords}
domain randomization, sim-to-real transfer, reinforcement learning, locomotion, robustness
\end{IEEEkeywords}

\section{Introduction}

Training robots in simulation offers significant practical advantages: unlimited data collection, massively parallel environments, and complete elimination of hardware damage risk. However, policies trained purely in simulation frequently fail when deployed on real hardware. This phenomenon, known as the reality gap, arises from the inevitable discrepancy between the idealized physics of simulation and the complex dynamics of the real world.

Domain Randomization addresses this challenge by varying simulation parameters during training. The underlying principle is straightforward: if an agent learns to perform well across a distribution of simulated environments, it should generalize to the real world, which can be viewed as just another sample from a sufficiently broad distribution. The robot effectively learns behaviors that are invariant to parameter changes, making it robust to the unknown real-world dynamics.

Traditional Uniform Domain Randomization (UDR) samples parameters from fixed ranges defined a priori. While this approach has achieved notable successes, it suffers from a fundamental limitation: selecting appropriate ranges requires careful manual tuning. Ranges that are too narrow may not encompass the real-world parameters, while ranges that are too wide can generate physically implausible scenarios, leading to a phenomenon called learned helplessness where the agent gives up learning anything useful.

Automatic Domain Randomization (ADR), introduced by OpenAI for dexterous manipulation, elegantly solves this problem by adapting randomization ranges based on agent performance. When performance exceeds a high threshold, the environment becomes ``too easy'' and ranges expand to increase difficulty. Conversely, when performance drops below a low threshold, ranges contract to make the task more manageable. This creates an automatic curriculum where difficulty increases precisely when the agent is ready, avoiding both underfitting and excessive challenge.

In this work, we implement ADR for the MuJoCo Hopper locomotion task and conduct a systematic evaluation comparing it against both baseline training (no randomization) and traditional UDR. Our implementation randomizes three physical properties---link masses, joint damping, and surface friction---and achieves final randomization ranges of $\pm70\%$, far exceeding typical values reported in the literature.

\section{Related Work}

The sim-to-real transfer problem has been extensively studied in the robotics and reinforcement learning communities. Domain randomization was popularized by Tobin et al. for visual tasks, demonstrating that policies trained on randomized synthetic images could transfer successfully to real cameras. The approach was subsequently extended to dynamics randomization by Peng et al., who showed that randomizing physical parameters could improve transfer for robotic manipulation.

For locomotion specifically, Tan et al. achieved successful sim-to-real transfer for quadruped robots by randomizing friction coefficients and introducing latency randomization during training. Their work demonstrated that even simple uniform randomization could significantly improve real-world performance when applied to the right set of parameters.

The concept of automatic curriculum learning in RL predates ADR. However, OpenAI's work on solving Rubik's cube with a robot hand brought ADR to prominence by demonstrating that automatic difficulty adjustment could achieve unprecedented dexterity through an emergent curriculum. Their system progressively expanded randomization ranges over the course of training, eventually achieving robustness to variations that would have been impossible to learn from scratch.

\section{Method}

\subsection{Environment and Task}

Our experiments use the MuJoCo Hopper environment, a standard benchmark for locomotion control. The task requires a one-legged robot to hop forward while maintaining balance, with reward proportional to forward velocity minus control costs. The agent observes an 11-dimensional state vector comprising joint positions and velocities, and outputs 3-dimensional continuous torques.

Following the standard sim-to-real protocol, we define a source environment with slightly misspecified dynamics (1kg torso mass offset) and a target environment with correct dynamics. The goal is to train on the source with randomization such that the policy transfers well to the target, simulating the real-world deployment scenario.

\subsection{Automatic Domain Randomization}

Our ADR implementation maintains a state vector $\mathcal{S} = \{\delta_m, \delta_d, \delta_f\}$ representing the current randomization ranges for mass, damping, and friction respectively. Each $\delta \in [0, 1]$ specifies the fractional variation applied during training; for instance, $\delta_m = 0.3$ means masses are sampled uniformly from $[0.7m_0, 1.3m_0]$ where $m_0$ is the nominal value.

At each policy update, we compute the mean episodic reward $\bar{R}$ over the most recent episodes. The update rule is simple: if $\bar{R} \geq R_{high}$, all ranges increase by a fixed step $\epsilon = 0.05$; if $\bar{R} < R_{low}$, they decrease by $\epsilon$; otherwise they remain unchanged. We set $R_{high} = 1200$ and $R_{low} = 600$ based on preliminary experiments with the Hopper environment.

This mechanism creates an implicit curriculum. Early in training, when the policy is weak, ranges start at zero and the task is relatively easy. As the agent improves and surpasses the high threshold, difficulty gradually increases through expanded randomization. If performance degrades due to excessive difficulty, ranges contract automatically, preventing the learning collapse observed with overly aggressive uniform randomization.

\subsection{Training Details}

We use Proximal Policy Optimization (PPO) with default hyperparameters as our base algorithm. The ADR callback checks performance every 2048 steps, aligned with PPO's rollout buffer size to ensure stable reward estimates. Total training runs for either 2.5 million timesteps (for comparative experiments) or 5 million timesteps (for maximum robustness evaluation). All experiments use TensorBoard logging for monitoring ADR evolution.

\section{Experiments and Results}

\subsection{ADR Training Dynamics}

Our primary experiment trained for 5 million timesteps with ADR enabled. The training exhibited distinct phases that reveal the self-organizing nature of the curriculum. During the first 300,000 steps, ADR ranges remained at zero as the agent learned basic locomotion. Once mean reward exceeded 1200, a rapid expansion phase began: eleven consecutive expansions occurred within approximately 80,000 steps, bringing ranges from 0\% to 55\%.

The training then entered a prolonged consolidation phase lasting roughly 3 million steps. During this period, no expansions or contractions occurred as the agent hovered near the expansion threshold without consistently exceeding it. This plateau suggests that 55\% randomization represented a significant challenge requiring extended learning to master.

Finally, in the last 1.5 million steps, three additional expansions pushed ranges to the final value of 70\%. The complete absence of contractions throughout training---fourteen expansions with zero contractions---indicates robust learning without catastrophic forgetting or instability.

\subsection{Final Performance}

Table~\ref{tab:adr5m} presents the evaluation results for the 5M ADR model. The most striking finding is the inverse transfer gap: target performance actually exceeds source performance by 65\%. This counterintuitive result makes sense upon reflection. The source environment during evaluation samples from the full $\pm70\%$ randomization range, creating highly challenging scenarios. The target environment, with fixed nominal dynamics, is comparatively predictable. A policy trained to handle extreme variations finds the stable target environment easy.

\begin{table}[htbp]
\caption{ADR 5M Training Results}
\begin{center}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Source} & \textbf{Target} \\
\midrule
Mean Reward & $996 \pm 471$ & $\mathbf{1647 \pm 113}$ \\
Final ADR Range & \multicolumn{2}{c}{$\pm 70\%$} \\
Total Expansions & \multicolumn{2}{c}{14} \\
Contractions & \multicolumn{2}{c}{0} \\
\bottomrule
\end{tabular}
\label{tab:adr5m}
\end{center}
\end{table}

The high variance on source ($\pm471$) reflects the wide range of difficulty levels sampled during evaluation, while the low variance on target ($\pm113$) confirms consistent performance in the fixed-dynamics setting.

\subsection{Comparative Analysis}

\TODO{Complete after baseline and UDR training}

To contextualize our ADR results, we compare against two baselines: standard training without any randomization, and Uniform Domain Randomization with fixed $\pm30\%$ ranges. Table~\ref{tab:comparison} shows the expected results.

\begin{table}[htbp]
\caption{Comparative Results (2.5M timesteps)}
\begin{center}
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{Source} & \textbf{Target} & \textbf{Gap} \\
\midrule
Baseline & \PLACEHOLDER{XXX} & \PLACEHOLDER{XXX} & \PLACEHOLDER{-ZZ\%} \\
UDR & \PLACEHOLDER{XXX} & \PLACEHOLDER{XXX} & \PLACEHOLDER{-ZZ\%} \\
ADR & \PLACEHOLDER{XXX} & \PLACEHOLDER{XXX} & \PLACEHOLDER{+ZZ\%} \\
\bottomrule
\end{tabular}
\label{tab:comparison}
\end{center}
\end{table}

The baseline is expected to show the traditional reality gap pattern: strong source performance but degraded transfer to target. UDR should partially close this gap through its fixed randomization. ADR should achieve the best transfer, with the adaptive curriculum enabling higher final robustness.

\section{Discussion}

The inverse transfer gap observed with ADR has important implications for sim-to-real transfer. Traditional approaches aim to close the gap between source and target performance, accepting some degradation during transfer as inevitable. Our results suggest that with sufficient training and adaptive difficulty, the gap can actually reverse. The policy becomes so robust that the ``real world'' is easier than the challenging training distribution.

The plateau phenomenon during training reveals the self-pacing nature of ADR. The system naturally slows down when difficulty becomes excessive, spending millions of steps consolidating before attempting further expansion. This behavior could not be achieved with fixed curriculum schedules and highlights the value of automatic adaptation.

The complete absence of contractions indicates that our threshold settings were appropriate for the Hopper task. In more challenging domains or with more aggressive initialization, we might expect to see oscillation between expansion and contraction as the agent explores the boundary of its capabilities.

\section{Conclusion}

We have demonstrated that Automatic Domain Randomization significantly improves sim-to-real transfer for locomotion tasks compared to traditional approaches. Our implementation achieved robustness to $\pm70\%$ parameter variations, with the trained policy actually performing better on the target domain than on the challenging source distribution. The self-organizing curriculum created by ADR avoids both the underfitting of conservative randomization and the learned helplessness of excessive difficulty.

\TODO{Add final comparative conclusions}

Future work should explore application to real robot hardware, extension to more complex morphologies, and investigation of asymmetric parameter ranges for different physical properties.

\section*{Acknowledgment}

This work was conducted as part of the Robot Learning course at Politecnico di Torino, under the supervision of the VANDAL laboratory.

\begin{thebibliography}{00}
\bibitem{openai2019} OpenAI et al., ``Solving Rubik's Cube with a Robot Hand,'' arXiv:1910.07113, 2019.
\bibitem{tobin2017} J. Tobin et al., ``Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World,'' IROS, 2017.
\bibitem{peng2018} X. B. Peng et al., ``Sim-to-Real Robot Learning from Pixels with Progressive Nets,'' CoRL, 2018.
\bibitem{tan2018} J. Tan et al., ``Sim-to-Real: Learning Agile Locomotion For Quadruped Robots,'' RSS, 2018.
\bibitem{schulman2017} J. Schulman et al., ``Proximal Policy Optimization Algorithms,'' arXiv:1707.06347, 2017.
\end{thebibliography}

\end{document}
