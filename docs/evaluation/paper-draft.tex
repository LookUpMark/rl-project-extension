\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}

\begin{document}

\title{Automatic Domain Randomization for Robust Sim-to-Real Transfer in Locomotion Tasks}

\author{\IEEEauthorblockN{Marc'Antonio Lopez}
\IEEEauthorblockA{\textit{Department of Control and Computer Engineering} \\
\textit{Polytechnic University of Turin}\\
Turin, Italy \\
s336362@studenti.polito.it}
\and
\IEEEauthorblockN{Luigi Marguglio}
\IEEEauthorblockA{\textit{Department of Control and Computer Engineering} \\
\textit{Polytechnic University of Turin}\\
Turin, Italy \\
s332575@studenti.polito.it}
}

\maketitle

\begin{abstract}
Transferring reinforcement learning policies from simulation to the real world remains a fundamental challenge due to the mismatch between simulated and physical dynamics, commonly known as the reality gap. In this work, we implement and evaluate Automatic Domain Randomization (ADR) for the MuJoCo Hopper locomotion task. Unlike traditional approaches that use fixed randomization ranges, our method adaptively expands environmental difficulty based on agent performance, creating a natural curriculum that prevents both underfitting and learned helplessness. Through systematic experiments comparing five training configurations---Baseline, Uniform Domain Randomization (UDR), and ADR at 2.5M, 5M, and 10M timesteps---we find that UDR achieves the best transfer performance with a $+3.9\%$ positive transfer gap, while ADR 10M achieves near-perfect transfer stability ($-0.4\%$ gap) with significantly lower variance. Notably, ADR models reach robustness levels of $\pm60\%$ parameter variation. Our results demonstrate the importance of balancing robustness with task performance for effective sim-to-real transfer.
\end{abstract}

\begin{IEEEkeywords}
domain randomization, sim-to-real transfer, reinforcement learning, locomotion, robustness
\end{IEEEkeywords}

\section{Introduction}

Training robots in simulation offers significant practical advantages: unlimited data collection, massively parallel environments, and complete elimination of hardware damage risk. However, policies trained purely in simulation frequently fail when deployed on real hardware. This phenomenon, known as the reality gap, arises from the inevitable discrepancy between the idealized physics of simulation and the complex dynamics of the real world.

Domain Randomization addresses this challenge by varying simulation parameters during training. The underlying principle is straightforward: if an agent learns to perform well across a distribution of simulated environments, it should generalize to the real world, which can be viewed as just another sample from a sufficiently broad distribution. The robot effectively learns behaviors that are invariant to parameter changes, making it robust to the unknown real-world dynamics.

Traditional Uniform Domain Randomization (UDR) samples parameters from fixed ranges defined a priori. While this approach has achieved notable successes, it suffers from a fundamental limitation: selecting appropriate ranges requires careful manual tuning. Ranges that are too narrow may not encompass the real-world parameters, while ranges that are too wide can generate physically implausible scenarios, leading to a phenomenon called learned helplessness where the agent gives up learning anything useful.

Automatic Domain Randomization (ADR), introduced by OpenAI for dexterous manipulation, elegantly solves this problem by adapting randomization ranges based on agent performance. When performance exceeds a high threshold, the environment becomes ``too easy'' and ranges expand to increase difficulty. Conversely, when performance drops below a low threshold, ranges contract to make the task more manageable. This creates an automatic curriculum where difficulty increases precisely when the agent is ready, avoiding both underfitting and excessive challenge.

In this work, we implement ADR for the MuJoCo Hopper locomotion task and conduct a systematic evaluation comparing it against both baseline training (no randomization) and traditional UDR. Our implementation randomizes three physical properties---link masses, joint damping, and surface friction---and investigates how training duration affects the robustness-performance tradeoff. All experiments use a fixed random seed (42) to ensure reproducibility.

\section{Related Work}

The sim-to-real transfer problem has been extensively studied in the robotics and reinforcement learning communities. Domain randomization was popularized by Tobin et al.~\cite{tobin2017} for visual tasks, demonstrating that policies trained on randomized synthetic images could transfer successfully to real cameras. The approach was subsequently extended to dynamics randomization by Peng et al.~\cite{peng2018}, who showed that randomizing physical parameters could improve transfer for robotic manipulation.

For locomotion specifically, Tan et al.~\cite{tan2018} achieved successful sim-to-real transfer for quadruped robots by randomizing friction coefficients and introducing latency randomization during training. Their work demonstrated that even simple uniform randomization could significantly improve real-world performance when applied to the right set of parameters.

The concept of automatic curriculum learning in RL predates ADR. However, OpenAI's work on solving Rubik's cube with a robot hand~\cite{openai2019} brought ADR to prominence by demonstrating that automatic difficulty adjustment could achieve unprecedented dexterity through an emergent curriculum. Their system progressively expanded randomization ranges over the course of training, eventually achieving robustness to variations that would have been impossible to learn from scratch.

\section{Method}

\subsection{Environment and Task}

Our experiments use the MuJoCo Hopper environment, a standard benchmark for locomotion control. The task requires a one-legged robot to hop forward while maintaining balance, with reward proportional to forward velocity minus control costs. The agent observes an 11-dimensional state vector comprising joint positions and velocities, and outputs 3-dimensional continuous torques.

Following the standard sim-to-real protocol, we define a source environment with slightly misspecified dynamics (1kg torso mass offset) and a target environment with correct dynamics. The goal is to train on the source with randomization such that the policy transfers well to the target, simulating the real-world deployment scenario.

\subsection{Automatic Domain Randomization}

Our ADR implementation maintains a state vector $\mathcal{S} = \{\delta_m, \delta_d, \delta_f\}$ representing the current randomization ranges for mass, damping, and friction respectively. Each $\delta \in [0, 1]$ specifies the fractional variation applied during training; for instance, $\delta_m = 0.3$ means masses are sampled uniformly from $[0.7m_0, 1.3m_0]$ where $m_0$ is the nominal value.

At each policy update, we compute the mean episodic reward $\bar{R}$ over the most recent episodes. The update rule is simple: if $\bar{R} \geq R_{high}$, all ranges increase by a fixed step $\epsilon = 0.05$; if $\bar{R} < R_{low}$, they decrease by $\epsilon$; otherwise they remain unchanged.

The threshold values $R_{high} = 1200$ and $R_{low} = 600$ were selected based on observed training dynamics. During early training phases, we noted that untrained policies achieve rewards below 500, while policies that have learned stable locomotion consistently exceed 1000. We set $R_{high} = 1200$ to trigger expansion only when the policy demonstrates solid mastery of the current difficulty level. The lower threshold $R_{low} = 600$ was chosen to detect performance degradation before complete collapse, allowing the system to reduce difficulty when the agent struggles. The 600-point neutral zone between thresholds provides stability, preventing oscillation between expansion and contraction during normal training fluctuations.

This mechanism creates an implicit curriculum. Early in training, when the policy is weak, ranges start at zero and the task is relatively easy. As the agent improves and surpasses the high threshold, difficulty gradually increases through expanded randomization. If performance degrades due to excessive difficulty, ranges contract automatically, preventing the learning collapse observed with overly aggressive uniform randomization.

\subsection{Training Details}

We use Proximal Policy Optimization (PPO)~\cite{schulman2017} with default hyperparameters as our base algorithm. The ADR callback checks performance every 2048 steps, aligned with PPO's rollout buffer size to ensure stable reward estimates. All experiments use a fixed random seed (42) for reproducibility. We train five different configurations for comparison:

\begin{itemize}
    \item \textbf{Baseline}: No domain randomization, 2.5M timesteps
    \item \textbf{UDR}: Uniform randomization with fixed $\pm30\%$ ranges, 2.5M timesteps
    \item \textbf{ADR 2.5M}: Automatic Domain Randomization, 2.5M timesteps
    \item \textbf{ADR 5M}: Automatic Domain Randomization, 5M timesteps
    \item \textbf{ADR 10M}: Automatic Domain Randomization, 10M timesteps
\end{itemize}

All experiments use TensorBoard logging for monitoring training progress and ADR evolution. Each model is evaluated on 50 episodes per environment to ensure statistical significance.

\section{Experiments and Results}

\subsection{ADR Training Dynamics}

Figure~\ref{fig:training} shows the ADR range expansion over training. Both the 2.5M and 5M runs achieved a final range of $\pm60\%$, while the 10M run plateaued at $\pm40\%$. This counterintuitive result suggests that longer training does not necessarily lead to higher ADR ranges; instead, the stochastic nature of the expansion process and the agent's learning trajectory play significant roles.

The training exhibited distinct phases across all ADR runs. During the initial phase, ranges remained at zero as the agent learned basic locomotion. Once mean reward exceeded 1200, rapid expansion began. The 2.5M and 5M runs experienced aggressive early expansion, while the 10M run showed more gradual, steady growth with eventual stabilization.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{figures/training_curves.png}
\caption{Left: ADR range expansion over training for different training durations. Right: Mean episode reward during training, with the dashed line indicating the expansion threshold (1200).}
\label{fig:training}
\end{figure}

\subsection{Comparative Analysis}

Table~\ref{tab:comparison} presents the main evaluation results across all five training configurations. Each model was evaluated on 50 episodes in both source and target environments with clean (non-randomized) dynamics for fair comparison.

\begin{table}[htbp]
\caption{Comparative Results (50 evaluation episodes, seed=42)}
\begin{center}
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{Source} & \textbf{Target} & \textbf{Gap} \\
\midrule
Baseline & $1778 \pm 65$ & $1169 \pm 95$ & $-34.2\%$ \\
UDR & $1660 \pm 10$ & $\mathbf{1725 \pm 34}$ & $\mathbf{+3.9\%}$ \\
ADR 2.5M & $1567 \pm 7$ & $1533 \pm 133$ & $-2.1\%$ \\
ADR 5M & $1013 \pm 224$ & $781 \pm 139$ & $-22.9\%$ \\
ADR 10M & $1462 \pm 39$ & $1457 \pm 145$ & $-0.4\%$ \\
\bottomrule
\end{tabular}
\label{tab:comparison}
\end{center}
\end{table}

Several key observations emerge from these results. First, the baseline exhibits the classical reality gap with a significant $-34.2\%$ degradation when transferring to the target domain. This confirms that without any form of domain randomization, policies overfit to the specific dynamics of the training environment.

Second, UDR achieves the best transfer performance with a $+3.9\%$ positive gap and remarkably low source variance ($\pm10$). The fixed $\pm30\%$ randomization range appears to be well-calibrated for this task, providing sufficient robustness without excessive difficulty.

Third, ADR 10M achieves near-perfect transfer stability with only $-0.4\%$ gap, making it the most stable transfer option despite not having the highest absolute performance. The low variance ($\pm39$ on source) indicates consistent behavior.

Fourth, ADR 5M shows unexpectedly poor performance ($-22.9\%$ gap). Despite reaching $\pm60\%$ ADR range, the model struggled during evaluation, possibly due to the aggressive randomization affecting its ability to perform well on fixed dynamics.

Figure~\ref{fig:comparison} provides a visual comparison of the final performance across all methods.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{figures/final_performance.png}
\caption{Final performance comparison across training methods. Green bars show source (training) environment performance, red bars show target (transfer) environment performance. Error bars indicate standard deviation over 50 evaluation episodes.}
\label{fig:comparison}
\end{figure}

\subsection{Robustness Evaluation}

To better understand the behavior of ADR models under randomized conditions, we evaluated them with randomized source environments (UDR enabled), simulating the challenging conditions they were trained under. Table~\ref{tab:robustness} presents these results.

\begin{table}[htbp]
\caption{ADR Robustness Evaluation (randomized source, 50 episodes)}
\begin{center}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Source (UDR)} & \textbf{Target} & \textbf{Gap} \\
\midrule
ADR 2.5M & $1542 \pm 125$ & $1541 \pm 124$ & $-0.1\%$ \\
ADR 5M & $804 \pm 167$ & $743 \pm 110$ & $-7.6\%$ \\
ADR 10M & $1488 \pm 149$ & $\mathbf{1533 \pm 126}$ & $\mathbf{+3.0\%}$ \\
\bottomrule
\end{tabular}
\label{tab:robustness}
\end{center}
\end{table}

Under randomized evaluation, ADR 10M shows positive transfer gap ($+3.0\%$), demonstrating excellent robustness when evaluated in conditions similar to its training environment. ADR 2.5M achieves near-perfect balance ($-0.1\%$), while ADR 5M continues to show degraded performance, confirming that its high ADR range may have led to overly conservative learned behaviors.

\section{Discussion}

Our experiments reveal nuanced insights about domain randomization strategies. The most significant finding is that UDR with well-calibrated fixed ranges ($\pm30\%$) outperforms ADR on this task, achieving $+3.9\%$ positive transfer with minimal variance. This suggests that when appropriate ranges are known, the simplicity of UDR may be preferable.

However, ADR 10M demonstrates a compelling advantage: exceptional transfer stability ($-0.4\%$ gap) with low variance. For applications where consistent performance across deployment conditions is critical, ADR 10M provides the most reliable behavior despite lower absolute reward.

The poor performance of ADR 5M ($-22.9\%$ gap) is surprising given its $\pm60\%$ ADR range matching ADR 2.5M. This may be attributed to the specific learning trajectory during training, where the policy may have developed overly conservative strategies that work under extreme randomization but underperform on fixed dynamics.

The baseline's severe reality gap ($-34.2\%$) confirms the fundamental importance of domain randomization for sim-to-real transfer. Without any form of parameter variation during training, policies fail catastrophically when dynamics differ from the training environment.

ADR range expansion is not strictly monotonic with training duration: ADR 10M achieved only $\pm40\%$ while ADR 2.5M and 5M both reached $\pm60\%$. This indicates that the expansion process is highly dependent on the stochastic training trajectory rather than training duration alone.

\section{Conclusion}

We have conducted a systematic evaluation of Automatic Domain Randomization for locomotion tasks, comparing it against baseline and uniform randomization approaches across multiple training durations. All experiments used a fixed random seed (42) for reproducibility. Our key findings are:

\begin{enumerate}
    \item UDR achieves the best transfer performance with $+3.9\%$ positive gap when well-calibrated ranges are available.
    \item ADR 10M provides the most stable transfer ($-0.4\%$ gap) with low variance, ideal for applications requiring consistent deployment behavior.
    \item ADR models achieve robustness levels up to $\pm60\%$ parameter variation, though higher ranges do not guarantee better transfer.
    \item The baseline exhibits significant reality gap ($-34.2\%$), confirming the necessity of domain randomization for successful transfer.
\end{enumerate}

These results suggest that practitioners should carefully consider their specific requirements: UDR for maximum transfer performance when ranges are known, and ADR 10M for maximum stability when consistent behavior is paramount.

Future work should explore application to real robot hardware, investigation of asymmetric parameter ranges for different physical properties, and development of early stopping criteria based on transfer performance estimation.

\section*{Acknowledgment}

This work was conducted as part of the Robot Learning course at Politecnico di Torino, under the supervision of the VANDAL laboratory.

\begin{thebibliography}{00}
\bibitem{openai2019} OpenAI et al., ``Solving Rubik's Cube with a Robot Hand,'' arXiv:1910.07113, 2019.
\bibitem{tobin2017} J. Tobin et al., ``Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World,'' IROS, 2017.
\bibitem{peng2018} X. B. Peng et al., ``Sim-to-Real Robot Learning from Pixels with Progressive Nets,'' CoRL, 2018.
\bibitem{tan2018} J. Tan et al., ``Sim-to-Real: Learning Agile Locomotion For Quadruped Robots,'' RSS, 2018.
\bibitem{schulman2017} J. Schulman et al., ``Proximal Policy Optimization Algorithms,'' arXiv:1707.06347, 2017.
\end{thebibliography}

\end{document}
